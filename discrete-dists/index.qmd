---
title: "Discrete Distributions"
subtitle: "Linking Probability and Data"  
author: "Robert W. Walker"
format: 
   revealjs:
     theme: [custom.scss]
     scrollable: true
     logo: AGSMlogo.jpeg
     footer: "DADM: Discrete Distributions"
     html-math-method: katex
     incremental: true
     slide-number: c/t
     transition: zoom
     multiplex: true
     preview-links: true
     code-fold: true
---

# Discrete Distributions

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=TRUE, comment=NA, prompt=FALSE, fig.retina = 3, eval=TRUE, cache=TRUE)
```

```{r}
library(tidyverse)
theme_set(theme_minimal())
```

## Probability Distributions of Two Forms

Our core concept is a probability distribution just as above.  These come in two forms for two types [discrete (qualitative)] and continuous (quantitative)] and can be either:


- Assumed, or
- Derived

## [The Poster](https://github.com/robertwwalker/DADMStuff/raw/master/Distribution-Poster.pdf) and Examples

- Distributions are nouns.  

- Sentences are incomplete without verbs -- parameters.

- We need both; it is for this reason that the former slide is true.  

- We do not always have a grounding for either the name or the parameter.

- For now, we will work with **univariate** distributions though **multivariate** distributions do exist.

## Functions

Probability distributions are mathematical formulae expressing likelihood for some set of qualities or quantities.  

- They have names: nouns.  
- They also have verbs: parameters.  

Like a proper English sentence, both are required.


## Models

### What is a model?

. . .

For our purposes, it is a systematic description of a phenomenon that shares important and essential features of that phenomenon.  Models frequently give us leverage on problems in the absence of alternative approaches.

## One Review Problem: Six Sigma

$6\sigma$ is a widely used tool in TQM [total quality management].  But $6\sigma$ isn't actually $6\sigma$.  The famous mantra that goes with it is 3.4 dipmo [defects per million opportunities].  This means that the outer limit [and it only considers one-sided/tailed] on defects is 0.0000034.

## Radiant on the problem. {.smaller}

:::: {.columns}

::: {.column width="40%"}
```{r}
options(scipen=8)
library(radiant)
result <- prob_norm(mean = 0, stdev = 1, plb = 0.0000034)
summary(result, type = "probs")
```

$6\sigma$ is only $4.5$.

![Six Sigma from Wikipedia](img/SixSigmaDrift.png)

:::

::: {.column width="60%"}
```{r}
plot(result, type = "probs") + theme_minimal()
```
:::

::::


## Today's **Models**

- The *Poisson* will be defined an arrival rate $\lambda$ -- lambda.   
- **Bernoulli trials**: Two outcomes occur with probability $\pi$ and $1-\pi$.    
    + The *binomial distribution* will be defined by a number of trials $n$ and a probability $\pi$.  
    + The *geometric distribution* defines the first success of $n$ trials with probability $\pi$.  
    + The *negative binomial distribution* defines the probability of $k$ successes in $n$ trials with probability $\pi$.  It is related to the **Poisson**.


## Events: The Poisson

![Poisson](https://github.com/robertwwalker/DADMStuff/raw/master/Poisson.jpg)



Take a binomial with $p$ very small and let $n \rightarrow \infty$.  We get the Poisson distribution ($y$) given an arrival rate $\lambda$ specified in events per period.

$$f(y|\lambda) = \frac{\lambda^{y}e^{-\lambda}}{y!}$$



## Examples: The Poisson

- Walk in customers
- *Emergency Room Arrivals*
- Births, deaths, marriages
- *Prussian Cavalry Deaths by Horse Kick*
- Fish?



## Air Traffic Controllers

**FAA Decision: Expend or do not expend scarce resources investigating claimed staffing shortages at the Cleveland Air Route Traffic Control Center.**

*Essential facts: The Cleveland ARTCC is the US's busiest in routing cross-country air traffic.  In mid-August of 1998, it was reported that the first week of August experienced 3 errors in a one week period; an error occurs when flights come within five miles of one another by horizontal distance or 2000 feet by vertical distance.  The Controller's union claims a staffing shortage though other factors could be responsible.  21 errors per year (21/52 [`r 21/52`] errors per week) has been the norm in Cleveland for over a decade.*

## Some Questions {.smaller}

1. Plot a histogram of 1000 random weeks.  NB: *pois* is the noun with no default for $\lambda$ -- the arrival rate.

:::: {.columns}

::: {.column width="50%"}

```{r}
DF <- data.frame(Close.Calls = rpois(1000, 21/52))
ggplot(DF) + aes(x=Close.Calls) + geom_histogram()
```

:::

::: {.column width="50%"}

```{r}
ggplot(DF) + aes(x=Close.Calls) + stat_ecdf(geom="step")
```

:::

::::

## Another Look

:::: {.columns}

::: {.column width="50%"}

+ Plot a sequence on the x axis from 0 to 5 and the probability of that or fewer incidents along the y.  *seq(0,5)*

:::
  
::: {.column width="50%"}

```{r}
DF <- data.frame(x=0:5, y=ppois(0:5, 21/52))
ggplot(DF) + aes(x=x, y=y) + geom_col()
```

:::
  
::::

## Decision

2. What would you do and why?  *Not impossible*. The probability of this number or greater is `r 1-ppois(2, 21/52)`

## A Wrinkle

:::: {.columns}

::: {.column width="40%"}

3. After analyzing the initial data, you discover that the first two weeks of August have experienced 6 errors.  What would you now decide?  

:::
  
::: {.column width="60%"}

*Well, once is `r ppois(2, 21/52, lower.tail=FALSE)`.*  Twice, at random, is that squared.  We have a problem.  Or...

```{r}
data.frame(Errors = seq(0,8)) %>% mutate(Pr = dpois(Errors, 21/26)) %>% ggplot() + aes(x=Errors, y=Pr) + geom_col() + theme_minimal()
```
:::
  
::::



## Deaths by Horse Kick in the Prussian cavalry?

:::: {.columns}
::: {.column width="40%"}
```{r VB1}
library(vcd)
data(VonBort)
head(VonBort)
```

In Von Bortkiewicz's data, the average number of deaths by horse kick is:

```{r}
mean(VonBort$deaths)
```

:::
::: {.column width="60%"}

```{r VBG1, echo=FALSE}
par(mfrow=c(1,2))
hist(VonBort$deaths, main="Deaths by Horse Kick: 1875-94", xlab="Deaths")
plot(x=seq(0,5,1),y=1-ppois(seq(0,5,1),0.7), ylim=c(0,1), xlab="Expected Deaths", ylab="Pr")
```
:::
::::


# Bernoulli Trials

## The Generic Bernoulli Trial

Suppose the variable of interest is discrete and takes only two values: yes and no.  For example, is a customer satisfied with the outcomes of a given service visit?  

For each individual, because the probability of yes (1) $\pi$ and no (0) 1-$\pi$ must sum to one, we can write:

$$f(x|\pi) = \pi^{x}(1-\pi)^{1-x}$$


# Binomial Distribution

For multiple identical trials, we have the Binomial:

$$f(x|n,\pi) = {n \choose k} \pi^{x}(1-\pi)^{n-x}$$
where $${n \choose k} = \frac{n!}{(n-k)!}$$


## The Binomial

![BinomialR](https://github.com/robertwwalker/DADMStuff/raw/master/BinomialR.PNG)

## Scottish Pounds

*Informal surveys suggest that 15% of Essex shopkeepers will not accept Scottish pounds.  There are approximately 200 shops in the general High Street square.*

1. Draw a plot of the distribution and the cumulative distribution of shopkeepers that do not accept Scottish pounds.

```{r}
Scots <- data.frame(Potential.Refusers = 0:200) %>% mutate(Prob = round(pbinom(Potential.Refusers, size=200, 0.15), digits=4))
Scots %>% ggplot() + aes(x=Potential.Refusers, y=Prob) + geom_point() + labs(x="Refusers", y="Prob. of x or Less Refusers") + theme_minimal() -> Plot1
Plot1
```


## A Nicer Plot

:::: {.columns}
::: {.column width="40%"}

```{r, eval=FALSE}
library(plotly)
p <- ggplotly(Plot1)
p
```

:::
::: {.column width="60%"}

```{r, echo=FALSE}
library(plotly)
p <- ggplotly(Plot1)
p
```
        
:::
::::

## More Questions About Scottish Pounds

2. What is the probability that 24 or fewer will not accept Scottish pounds?

```{r}
pbinom(24, 200, 0.15)
```



3. What is the probability that 25 or more shopkeepers will not accept Scottish pounds?

```{r}
1-pbinom(24, 200, 0.15)
```

## Even More Scottish Pounds

4. With probability 0.9 [90 percent], XXX or fewer shopkeepers will not accept Scottish pounds.

```{r}
qbinom(0.9, 200, 0.15)
```


## The Median is a Binomial with p=0.5 {.smaller .nonincremental}

Interestingly, any given observation has a 50-50 chance of being *over* or *under* the median.  Suppose that I have five datum.  

:::: {.columns}

::: {.column width="40%"}

1. What is the probability that all are under?

```{r}
pbinom(0,size=5, p=0.5)
```

2. What is the probability that all are over?

```{r}
dbinom(5,size=5, p=0.5)
```

3. What is the probability that the median is somewhere in between our smallest and largest sampled values?

**Everything else**.

$$ 1 - 0.03125 - 0.03125 = 0.9375 $$
:::
  
::: {.column width="60%"}



```{r fig.width = 7, fig.height = 4.31, dpi = 96}
result <- prob_binom(n = 5, p = 0.5, lb = 1, ub = 4)
plot(result) + theme_minimal()
```


:::
  
::::



## The Rule of Five

- This is called the *Rule of Five* by Hubbard in his *How to Measure Anything*.



## Geometric Distributions

How many failures before the first success?  Now defined exclusively by $p$.  In each case, $(1-p)$ happens $k$ times.  Then, on the $k+1^{th}$ try, p.  Note 0 failures can happen...

$$Pr(y=k) = (1-p)^{k}p$$



## Example: Entrepreneurs

:::: {.columns}

::: {.column width="50%"}

Suppose any startup has a $p=0.1$ chance of success.  How many failures?

```{r GP1, echo=FALSE}
par(mfrow=c(1,2))
plot(x=seq(0,30,1),y=dgeom(seq(0,30,1),0.1), xlab="Failures", ylab="Pr(Exactly XXX Failures)", main="Pr(Success)=0.1", type="h")
plot(x=seq(0,30,1),y=pgeom(seq(0,30,1),0.1), xlab="Failures", ylab="Pr(Less or XXX Failures)", main="Pr(Success)=0.1", type="h")
```
:::
  
::: {.column width="50%"}

Suppose any startup has a $p=0.1$ chance of success.  How many failures for the average/median person?

```{r}
qgeom(0.5,0.1)
```
:::
  
::::

## Example: Entrepreneurs {.smaller}

:::: {.columns}

::: {.column width="35%"}
[Geometric] Plot 1000 random draws of "How many vendors until one refuses my Scottish pounds?"

```{r}
Geoms.My <- data.frame(Vendors=rgeom(1000, 0.15))
Geoms.My %>% ggplot() + aes(x=Vendors) + geom_histogram(binwidth=1)
```
:::
  
::: {.column width="65%"}
We could also do something like.

```{r, fig.dim = c(5, 5)}
Plotly.Me <- data.frame(Refuseniks = seq(0,60), Pr = pgeom(seq(0,60), 0.15)) %>% ggplot() + aes(x=Refuseniks, y=Pr) + geom_col()
ggplotly(Plotly.Me)
```

:::
  
::::


## Negative Binomial Distributions

How many failures before the $r^{th}$ success?  In each case, (1-p) happens $k$ times.  Then, on the $k+1^{th}$ try, we get our $r^{th}$ p.  Note 0 failures can happen...

$$Pr(y=k) = {k+r-1 \choose r-1}(1-p)^{k}p^{r}$$

## Needed Sales


:::: {.columns}

::: {.column width="30%"}
I need to make 5 sales to close for the day.  How many potential customers will I have to have to get those five sales when each customer purchases with probability 0.2.


```{r}
library(patchwork)
DF <-  data.frame(Customers = c(0:70)) %>% 
  mutate(m.Customers = dnbinom(Customers, size=5, prob=0.2), 
         p.Customers = pnbinom(Customers, size=5, prob=0.2)) 
pl1 <- DF %>% ggplot() + aes(x=Customers) + geom_line(aes(y=p.Customers))  + theme_minimal()
pl2 <- DF %>% ggplot() + aes(x=Customers) + geom_point(aes(y=m.Customers)) + theme_minimal()
```

:::
  
::: {.column width="70%"}

```{r, echo=FALSE, fig.dim=c(5,5)}
pl1 + pl2
```

:::
  
::::

## Simulation: A Powerful Tool

In this last example, I was concerned with sales.  I might also want to generate revenues because I know the rough mean and standard deviation of sales.  Combining such things together forms the basis of a Monte Carlo simulation.

Some of the basics are covered in a swirl on *simulation*.


## An Example {.smaller}

:::: {.columns}

::: {.column width="30%"}


Customers arrive at a rate of 7 per hour.
You convert customers to buyers at a rate of 85%.
Buyers spend, on average 600 dollars with a standard deviation of 150 dollars.

```{r}
Sim <- 1:1000
Customers <- rpois(1000, 7)
Buyers <- rbinom(1000, size=Customers, prob = 0.85)
Data <- data.frame(Sim, Buyers, Customers)
Data <- Data %>% group_by(Sim) %>% mutate(Revenue = sum(rnorm(Buyers, 600, 150))) %>% ungroup()
```

:::
  
::: {.column width="70%"}



```{r Plot1, echo=FALSE, fig.dim=c(5,5)}
library(patchwork)
p1 <- ggplot(Data) + aes(x=Customers) + geom_histogram(binwidth = 1) + theme_minimal()
p2 <- ggplot(Data) + aes(x=Buyers) + geom_histogram(binwidth=1) + theme_minimal()
p3 <- ggplot(Data) + aes(x=Revenue) + geom_density() + theme_minimal()
(p1 + p2) / p3
```

:::
  
::::


## A Summary of Distributions

Distributions are how variables and probability relate.  They are a graph that we can enter in two ways.  From the probability side to solve for values or from values to solve for probability.  It is always a function of the graph.

Distributions generally have to be sentences. 

- The name is a noun but it also has 
- parameters -- verbs -- that makes the noun tangible.


